
https://docs.google.com/presentation/d/14hIWHcG_RWgfteoqACD2Z4zYgbdD3BYiYO0yLgRoL8k/edit#slide=id.g25990649439_0_0



Hello again!
Working with the file system is covered in the fs module, which we saw in the previous two movies. 
((((("welcome back.
in the previous two videos we learned about the fs module for working with the file system.))))))

Let's use the same FS module from the last video and investigate the streams innode here.Let me refresh your memory, JavaScript, because I quickly covered what streams are in a prior video.
((((("in this video Let's rely on the same FS module and learn about streams innode.js.
a few videos earlier I briefly explained what are streams let me help you recollect now.))))))

Information is said to be "streaming" when it is being transmitted from one location to another.
for instance, the flow of information between two files on the same computer.
(((((" a stream is a sequence of data that is being moved from one point to another over time.

// click

for example a stream of data being transferred from one file to another within the same computer.))))))

Instead of waiting for all of the data to be ready at once, the concept is to work with smaller pieces of data. 
((((("the idea is to work with data in chunks instead of waiting for the entire data to be available at once.))))))

You can copy the contents of file A into file B without waiting for file A's whole contents to be stored in temporary memory.   
((((("if you're transferring file contents from file a to file B you don't have to wait for entire file a content to be saved in temporary memory before moving it into file B.))))))

Transferring the data in smaller increments over a longer period of time helps conserve memory.
There is a pre-installed node module called stream that works similarly to the event emitter class. 
((((("instead the content is transferred in chunks over time which prevents unnecessary memory usage.
stream is in fact a built-in node module that inherits from the event emitter class.))))))

"However, we rarely make direct use of streams.
Streams are an integral part of the internal workings of other modules. 
((((("but we rarely use streams directly.
other modules internally use streams for their functioning.))))))


1:34

## file2.txt created

Let's return to our code and see how the fs module reads and writes data using streams.
I've made a blank text file, file2.txt, to serve as a starting point for our work together. 
((((("let's go back to vs code and understand how the fs module uses streams to read and write data.
to help us get started I’ve created file2.txt which is currently empty.))))))


### const fs = require("node:fs")

The information in file.txt will be copied into file2.txt.
To begin, bring in the FS module into index.js. 
((((("we are going to transfer the contents from file.txt to file2.txt.
in index.js at the top import the FS module.))))))

#### const readableStream = fs.createReadStream("./file.txt",{
	encoding: "uft-8",
})

The fs module's construct read stream function provides a readable stream that may be used to read data.
This procedure takes the file path as its input, thus cons readable stream is the same as FS dot construct read stream. 
((((("next to read data we use a readable stream made available using the create read stream method on the fs module.
so cons readable stream is equal to FS dot create read stream and this method accepts the file path as the ))))))

first argument and a second parameter of type Options Object.
Dot slash filename.txt is the path. 
((((("first argument and an options object as a second argument.
path is dot slash file Dot txt.))))))


((((("and for the second argument which is the options object let's set encoding to UDF Aid.
we have now created a readable stream to read data in chunks from file.txt.))))))

#### const writableStream = fs.createWriteStream("./file2.txt");

in addition, let's change the encoding of the choices object, the second parameter, to UDF Aid.
Now that we've made a stream, we can read file.txt in pieces.
((((("next let's create a writable stream to write data in chunks to file2.txt.
so const writable stream.))))))

is equivalent to "FS.create" "write stream"
The location of the file we'll be writing to is specified as the first parameter once more. 
((((("is equal to FS dot create write stream.
the first argument again is the path to the file to which we are going to write the contents.))))))

We now have a stream that can be read as well as written to. 
((((( we now have a readable stream and writable stream.))))))

In fact, streams are a subclass of the event emitter.
We've examined the notion of an event emitter before. 
((((("as it turns out streams extend from the event emitter class.
and event emitter is a concept we’ve already had a look at.))))))

#### readableStream.on("data", (chunk) => {
	console.log(chunk)
	writeableStream.write(chunk)
})

It enables us to attach receivers to events.
The display now sends out a data event that we can read. 
((((("it allows us to add listeners to events.
now the readable screen emits a data event to which we can listen.))))))

To put it another way, the data is the dot after the event's name.
together with a callback function to be invoked when the data event occurs.
((((("so readable stream dot on the event name is data.
and we specify a callback function that should be executed on the data event.))))))


//// cursor on chuck ()
A piece of information is sent directly to this callback function, which acts as a listener.
and it will be recorded in the console. 
((((("this callback function is the listener which automatically receives a chunk of data.
and we are going to log that to the console.))))))


4.24

In the next line, we will also use the writable stream to write to file2.txt.
chunk writing at writable stream dot write.
((((("in the next line we are also going to write to file2.txt using the writable stream.
writable stream dot write and we write the chunk.))))))

and that is about all there is to it.
You may now save the file and use node index if you want to.
((((("and that is pretty much it.
if you now save the file and run node index.))))))

As far as we can tell, the contents of the whole file are contained in our piece.
The identical information is saved in the second text file
((((("we can see our chunk is the entire file content hello Faraz Rawala.
the same is also written to file 2 Dot txt.))))))

Perhaps you're asking whether the whole file.txt isn't contained within the chunk.
Because the buffer used by streams has a default size of 64 kilobytes, this is the case.
((((("now you might be wondering isn't the chunk the entire file content of file.txt.
well yes it is and this is because the buffer that streams use has a default size of 64 kilobytes.))))))

The total number of characters in our file is 18, which is just 18 bytes.
hence all 18 bytes are included in the chunk.
((((("our file content has a total of 18characters which is just 18 bytes.
so the chunk contains the entire 18bytes.))))))

### highWaterMark:2


 To remedy this, we may provide users with more choices when reading data.
top of the range, thus we opted for the number 2.
((((("what we can do is add another option when reading data.
high water mark and we set it to 2.))))))

data is now handled in 2bytes increments.
The results of a node index 
((((("so we now deal with data in chunks of 2bytes.
if you run node index.))))))

You can see that for each chunk, only two characters are locked.
((((("you can see only two characters are locked at a time which corresponds to each chunk.))))))

Nonetheless, the download still contains the entire text, as was promised.
((((("the file though still contains the Full text as expected.))))))

hello, file2.txt now has the Evolution code that you requested.
((((("hello Faraz Rawala has been written tofile2.txt.))))))

Now that we're dealing with relatively little files, we don't see the need in doing this.
Streaming the data from one file to another will save time and space when working with megabyte-sized files.
((((("now we don't see the benefit here when working with a small file size.
but when you have large files that are megabytes in size streaming the data from one file to another will save))))))

you a ton of mental energy and time.
The fs module is only one of several that rely on stream processing.
((((("you a lot of time and memory.
the fs module is just one of the many modules that uses streams.))))))

The HTTP module is yet another illustration; more on it in the next subsection. 
((((("another example is the HTTP module which we will learn about in the upcoming section.))))))

The user's request for an HTTP resource is represented by a reading stream, while the server's answer is represented by a writable screen.
Actually, there are four distinct kinds of waterways. 
((((("HTTP request is a readable stream and HTTP response is a writable screen.
in fact there are four types of streams.))))))

channels via which data may be read and processed.
streams that can be written to that can receive data. 
((((("readable streams from which data can bread.
writeable streams to which we can write data.))))))

simultaneously reading and writable streams; a duplex stream.
 and lastly, transform streams, which may alter the information as it is being input and output.
((((("duplex streams that are both readable and writable.
 and finally transform streams that can modify or transform the data as it is written and read.))))))

Consider the common practice of reading text from a file as an illustration.
Using a file's writable stream to record text 
((((("as examples we can have reading from a file as readable stream.
writing to a file as a writable stream.))))))

a two-way stream via sockets.
and lastly, file compression, which allows you to both store and retrieve compressed data in the form of a transform stream within a file.
((((("sockets as a duplex stream.
and finally file compression where you can write compressed data and read decompress data to and from a file as a transform stream.))))))

For the time being, I'd like you to bear in mind that node.js has a concept of streams that lets us interact with data in pieces rather than massive volumes all at once. 
((((("for now I want you to keep in mind there exists the concept of streams in node.js which allows us to work with chunks of data rather than large amounts of data at once.))))))

OK, I guess I appreciate you taking the time to watch, and I hope to see you again soon.
((((("all right thank you for watching and I’ll see you in the next one.))))))
